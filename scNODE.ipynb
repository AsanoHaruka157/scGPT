{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550e247e",
   "metadata": {},
   "source": [
    "This is the present of scNODE for Single-cell transcriptomics time-series modelling and cell trajectory inference.\n",
    "\n",
    "Author:\n",
    "    Jiaqi Zhang <jiaqi_zhang2@brown.edu>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib for interactive display\n",
    "import matplotlib\n",
    "try:\n",
    "    # Try to use TkAgg backend for interactive plotting\n",
    "    matplotlib.use('TkAgg')\n",
    "except:\n",
    "    # Fallback to default backend if TkAgg is not available\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='plotting.visualization')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "project_root = str(pathlib.Path.cwd().resolve().parent)\n",
    "sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)  # Change the current working directory to the project root\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from BenchmarkUtils import loadSCData, tpSplitInd, tunedOurPars, splitBySpec\n",
    "from plotting.__init__ import *\n",
    "from plotting.visualization import plotPredAllTime, plotPredTestTime, computeDrift, plotStream, plotStreamByCellType\n",
    "from plotting.PlottingUtils import umapWithPCA, computeLatentEmbedding\n",
    "from optim.running import constructscNODEModel, scNODETrainWithPreTrain, scNODEPredict\n",
    "from optim.evaluation import globalEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       [ zebrafish ]                        \n",
      "Split type: three_interpolation\n",
      "[ Data=zebrafish | Split=three_interpolation ] Loading data...\n",
      "Dataset is loaded.\n",
      "Dataset is loaded.\n",
      "# tps=12, # genes=2000\n",
      "# cells=[311, 200, 1158, 1467, 5716, 1026, 4101, 6178, 5442, 7114, 1614, 4404]\n",
      "Train tps=tensor([ 0.,  1.,  2.,  3.,  5.,  7.,  9., 10., 11.])\n",
      "Test tps=tensor([4., 6., 8.])\n",
      "# tps=12, # genes=2000\n",
      "# cells=[311, 200, 1158, 1467, 5716, 1026, 4101, 6178, 5442, 7114, 1614, 4404]\n",
      "Train tps=tensor([ 0.,  1.,  2.,  3.,  5.,  7.,  9., 10., 11.])\n",
      "Test tps=tensor([4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# Load data and pre-processing\n",
    "# Specify the dataset: zebrafish, drosophila, wot\n",
    "# Representing ZB, DR, SC, repectively\n",
    "data_name= \"zebrafish\"\n",
    "print(\"[ {} ]\".format(data_name).center(60))\n",
    "split_type = \"three_interpolation\"\n",
    "print(\"Split type: {}\".format(split_type))\n",
    "ann_data, cell_tps, cell_types, n_genes, n_tps, all_tps = loadSCData(data_name, split_type)\n",
    "train_tps, test_tps = tpSplitInd(data_name, split_type)\n",
    "data = ann_data.X\n",
    "\n",
    "# Convert to torch project\n",
    "traj_data = [torch.FloatTensor(data[np.where(cell_tps == t)[0], :]) for t in range(1, n_tps + 1)]\n",
    "if cell_types is not None:\n",
    "    traj_cell_types = [cell_types[np.where(cell_tps == t)[0]] for t in range(1, n_tps + 1)]\n",
    "\n",
    "all_tps = list(all_tps)  # Convert to list\n",
    "train_data, test_data = splitBySpec(traj_data, train_tps, test_tps)\n",
    "tps = torch.FloatTensor(all_tps)\n",
    "train_tps = torch.FloatTensor(train_tps)\n",
    "test_tps = torch.FloatTensor(test_tps)\n",
    "n_cells = [each.shape[0] for each in traj_data]\n",
    "print(\"# tps={}, # genes={}\".format(n_tps, n_genes))\n",
    "print(\"# cells={}\".format(n_cells))\n",
    "print(\"Train tps={}\".format(train_tps))\n",
    "print(\"Test tps={}\".format(test_tps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeacac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "Pre-training VAE component...\n",
      "Pre-training iteration 20/100, Loss: 6.4668\n",
      "Pre-training iteration 40/100, Loss: 5.7228\n",
      "Pre-training iteration 60/100, Loss: 2.0397\n",
      "Pre-training iteration 80/100, Loss: 1.9819\n",
      "Pre-training iteration 100/100, Loss: 2.5954\n",
      "Pre-training iteration 20/100, Loss: 6.4668\n",
      "Pre-training iteration 40/100, Loss: 5.7228\n",
      "Pre-training iteration 60/100, Loss: 2.0397\n",
      "Pre-training iteration 80/100, Loss: 1.9819\n",
      "Pre-training iteration 100/100, Loss: 2.5954\n"
     ]
    }
   ],
   "source": [
    "# Model Defining and Training Settings\n",
    "NUM_EPOCHS = 10\n",
    "pretrain_lr = 1e-3\n",
    "latent_coeff = 1.0 # regularization coefficient: beta\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "act_name = \"relu\"\n",
    "n_sim_cells = 2000\n",
    "\n",
    "latent_dim, drift_latent_size, enc_latent_list, dec_latent_list = tunedOurPars(data_name, split_type) # use tuned hyperparameters\n",
    "latent_ode_model = constructscNODEModel(\n",
    "    n_genes, latent_dim=latent_dim,\n",
    "    enc_latent_list=enc_latent_list, dec_latent_list=dec_latent_list, drift_latent_size=drift_latent_size,\n",
    "    latent_enc_act=\"none\", latent_dec_act=act_name, drift_act=act_name,\n",
    "    ode_method=\"euler\"\n",
    ")\n",
    "\n",
    "# Start timing for progress tracking\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "train_start_time = time.time()\n",
    "\n",
    "# Import necessary modules for custom training loop\n",
    "import geomloss\n",
    "import itertools\n",
    "\n",
    "# Pre-training the VAE component\n",
    "latent_encoder = latent_ode_model.latent_encoder\n",
    "obs_decoder = latent_ode_model.obs_decoder\n",
    "all_train_data = torch.cat(train_data, dim=0)\n",
    "pretrain_iters = 100\n",
    "print(\"Pre-training VAE component...\")\n",
    "dim_reduction_params = itertools.chain(*[latent_encoder.parameters(), obs_decoder.parameters()])\n",
    "dim_reduction_optimizer = torch.optim.Adam(params=dim_reduction_params, lr=pretrain_lr, betas=(0.95, 0.99))\n",
    "latent_encoder.train()\n",
    "obs_decoder.train()\n",
    "for i in range(pretrain_iters):\n",
    "    rand_idx = np.random.choice(all_train_data.shape[0], size=batch_size, replace=False)\n",
    "    batch_data = all_train_data[rand_idx, :]\n",
    "    dim_reduction_optimizer.zero_grad()\n",
    "    latent_mu, latent_std = latent_encoder(batch_data)\n",
    "    latent_sample = latent_mu + torch.randn_like(latent_std) * latent_std\n",
    "    recon_obs = obs_decoder(latent_sample)\n",
    "    recon_loss = torch.mean((recon_obs - batch_data) ** 2)\n",
    "    recon_loss.backward()\n",
    "    dim_reduction_optimizer.step()\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Pre-training iteration {i+1}/{pretrain_iters}, Loss: {recon_loss.item():.4f}\")\n",
    "\n",
    "# Dynamic training setup\n",
    "optimizer = torch.optim.Adam(params=latent_ode_model.parameters(), lr=lr, betas=(0.95, 0.99))\n",
    "ot_solver = geomloss.SamplesLoss(\"sinkhorn\", p=2, blur=0.05, scaling=0.5, debias=True, backend=\"tensorized\")\n",
    "loss_list = []\n",
    "\n",
    "# Convert train_tps to list once for model forward\n",
    "train_tps_list = train_tps.detach().numpy().tolist() if isinstance(train_tps, torch.Tensor) else train_tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bc80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting main training loop...\n",
      "Epoch 1/10, Avg Loss: 51136.5554, Time: 0.5m\n",
      "Epoch 1/10, Avg Loss: 51136.5554, Time: 0.5m\n",
      "Epoch 5/10, Avg Loss: 26167.1351, Time: 1.9m\n",
      "Epoch 5/10, Avg Loss: 26167.1351, Time: 1.9m\n",
      "Epoch 10/10, Avg Loss: 23974.3964, Time: 3.9m\n",
      "\n",
      "Generating final predictions...\n",
      "======================================================================\n",
      "Training completed! Total epochs: 10\n",
      "Training time: 236.55 seconds (3.94 minutes)\n",
      "======================================================================\n",
      "Epoch 10/10, Avg Loss: 23974.3964, Time: 3.9m\n",
      "\n",
      "Generating final predictions...\n",
      "======================================================================\n",
      "Training completed! Total epochs: 10\n",
      "Training time: 236.55 seconds (3.94 minutes)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training process\n",
    "print(\"\\nStarting main training loop...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    n_iters = 0\n",
    "    \n",
    "    for iter_idx in range(100):  # iterations per epoch\n",
    "        rand_t_idx = np.random.choice(len(train_tps))\n",
    "        rand_idx = np.random.choice(train_data[rand_t_idx].shape[0], size=batch_size, replace=False)\n",
    "        batch_data = train_data[rand_t_idx][rand_idx, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        latent_ode_model.train()\n",
    "        # Model expects: forward(data_list, tps, batch_size=None)\n",
    "        # data should be a list with data at first timepoint\n",
    "        recon_obs, first_latent_dist, first_tp_data, latent_seq = latent_ode_model(\n",
    "            [batch_data], \n",
    "            torch.FloatTensor(train_tps_list)\n",
    "        )\n",
    "        \n",
    "        # Compute optimal transpost loss\n",
    "        ot_loss = 0\n",
    "        for t_idx, t in enumerate(train_tps):\n",
    "            pred_x = recon_obs[:, t_idx, :]\n",
    "            true_x = train_data[t_idx]\n",
    "            subsample_size = min(200, true_x.shape[0])\n",
    "            subsample_idx = np.random.choice(true_x.shape[0], subsample_size, replace=False)\n",
    "            ot_loss += ot_solver(pred_x, true_x[subsample_idx])\n",
    "        \n",
    "        latent_drift_loss = torch.mean((latent_seq[:, 1:, :] - latent_seq[:, :-1, :]) ** 2)\n",
    "        loss = ot_loss + latent_coeff * latent_drift_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append((loss.item(), ot_loss.item(), latent_drift_loss.item()))\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_iters += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / n_iters\n",
    "    elapsed = time.time() - train_start_time\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Avg Loss: {avg_loss:.4f}, Time: {elapsed/60:.1f}m\")\n",
    "\n",
    "print(\"\\nGenerating final predictions...\")\n",
    "latent_ode_model.eval()\n",
    "with torch.no_grad():\n",
    "    first_obs = train_data[0]\n",
    "    recon_obs, first_latent_dist, first_tp_data, latent_seq = latent_ode_model(\n",
    "        [first_obs], \n",
    "        torch.FloatTensor(train_tps_list)\n",
    "    )\n",
    "\n",
    "# Training summary\n",
    "train_duration = time.time() - train_start_time\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training completed! Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Training time: {train_duration:.2f} seconds ({train_duration/60:.2f} minutes)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_recon_obs = scNODEPredict(\n",
    "    latent_ode_model, \n",
    "    traj_data[0], \n",
    "    tps, \n",
    "    n_cells=n_sim_cells\n",
    ")  # (# cells, # tps, # genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f00e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21138\\AppData\\Local\\Temp\\ipykernel_22584\\3544298033.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()  # 先显示\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare true and reconstructed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21138\\Desktop\\快速访问\\scGPT\\scGPT\\plotting\\visualization.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\21138\\AppData\\Local\\Temp\\ipykernel_22584\\3544298033.py:29: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()  # 先显示所有时间点的对比图\n",
      "C:\\Users\\21138\\Desktop\\快速访问\\scGPT\\scGPT\\plotting\\visualization.py:57: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "  ax1.scatter(true_umap_traj[:, 0], true_umap_traj[:, 1], label=\"other\", c=gray_color, s=40, alpha=0.5)\n",
      "C:\\Users\\21138\\Desktop\\快速访问\\scGPT\\scGPT\\plotting\\visualization.py:58: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "  ax2.scatter(true_umap_traj[:, 0], true_umap_traj[:, 1], label=\"other\", c=gray_color, s=40, alpha=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving figure to figures/scNODE_Results.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21138\\AppData\\Local\\Temp\\ipykernel_22584\\3544298033.py:34: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()  # 先显示测试时间点的对比图\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute metrics...\n",
      "----------------------------------------------------------------------\n",
      "t = 4\n",
      "{'l2': 131.72967392419037, 'cos': 0.10999074341830356, 'corr': 0.11349175242797213, 'ot': 4088.7830120770877}\n",
      "----------------------------------------------------------------------\n",
      "t = 6\n",
      "{'l2': 131.72967392419037, 'cos': 0.10999074341830356, 'corr': 0.11349175242797213, 'ot': 4088.7830120770877}\n",
      "----------------------------------------------------------------------\n",
      "t = 6\n",
      "{'l2': 142.15103467594614, 'cos': 0.11507932860724181, 'corr': 0.11748344308314047, 'ot': 5456.958284737331}\n",
      "----------------------------------------------------------------------\n",
      "t = 8\n",
      "{'l2': 142.15103467594614, 'cos': 0.11507932860724181, 'corr': 0.11748344308314047, 'ot': 5456.958284737331}\n",
      "----------------------------------------------------------------------\n",
      "t = 8\n",
      "{'l2': 98.97907457164247, 'cos': 0.21959273818614994, 'corr': 0.22440769950576347, 'ot': 2920.9485990143908}\n",
      "{'l2': 98.97907457164247, 'cos': 0.21959273818614994, 'corr': 0.22440769950576347, 'ot': 2920.9485990143908}\n"
     ]
    }
   ],
   "source": [
    "# Visualization - Loss Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot([each[0] for each in loss_list])\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title(\"OT Term\")\n",
    "plt.plot([each[1] for each in loss_list])\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title(\"Dynamic Reg\")\n",
    "plt.plot([each[2] for each in loss_list])\n",
    "plt.xlabel(\"Dynamic Learning Iter\")\n",
    "plt.tight_layout()  # Optimize subplot layout\n",
    "plt.show()  # Display the figure interactively\n",
    "plt.savefig(\"figures/scNODE_Loss.png\")  # Save the figure to file\n",
    "plt.close()  # Explicitly close the figure to free memory\n",
    "\n",
    "# Visualization - 2D UMAP embeddings\n",
    "print(\"Compare true and reconstructed data...\")\n",
    "true_data = [each.detach().numpy() for each in traj_data]\n",
    "true_cell_tps = np.concatenate([np.repeat(t, each.shape[0]) for t, each in enumerate(true_data)])\n",
    "pred_cell_tps = np.concatenate([np.repeat(t, all_recon_obs[:, t, :].shape[0]) for t in range(all_recon_obs.shape[1])])\n",
    "reorder_pred_data = [all_recon_obs[:, t, :] for t in range(all_recon_obs.shape[1])]\n",
    "\n",
    "# Compute UMAP embeddings for visualization\n",
    "true_umap_traj, umap_model, pca_model = umapWithPCA(\n",
    "    np.concatenate(true_data, axis=0), \n",
    "    n_neighbors=50, \n",
    "    min_dist=0.1, \n",
    "    pca_pcs=50\n",
    ")\n",
    "pred_umap_traj = umap_model.transform(\n",
    "    pca_model.transform(np.concatenate(reorder_pred_data, axis=0))\n",
    ")\n",
    "\n",
    "# Display and save the first plot - All timepoints comparison\n",
    "plotPredAllTime(true_umap_traj, pred_umap_traj, true_cell_tps, pred_cell_tps)\n",
    "plt.show()  # Display all timepoints comparison\n",
    "\n",
    "# Display and save the second plot - Test timepoints comparison\n",
    "plotPredTestTime(\n",
    "    true_umap_traj, \n",
    "    pred_umap_traj, \n",
    "    true_cell_tps, \n",
    "    pred_cell_tps, \n",
    "    test_tps.detach().numpy(),\n",
    "    save_path=\"figures/scNODE_Results.png\"\n",
    ")\n",
    "plt.show()  # Display test timepoints comparison\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(\"Computing evaluation metrics...\")\n",
    "test_tps_list = [int(t) for t in test_tps]\n",
    "for t in test_tps_list:\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Timepoint t = {t}\")\n",
    "    pred_global_metric = globalEvaluation(\n",
    "        traj_data[t].detach().numpy(), \n",
    "        all_recon_obs[:, t, :]\n",
    "    )\n",
    "    print(pred_global_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
